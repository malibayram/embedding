{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08e773e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from turkish_tokenizer import TurkishTokenizer\n",
    "\n",
    "# Tokenizer'ı başlat\n",
    "tr_tokenizer = TurkishTokenizer()\n",
    "\n",
    "gemma_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-270m-it\")\n",
    "gemma_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-270m-it\")\n",
    "gemma_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "443284de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model from claude_model.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3Model(\n",
       "    (embed_tokens): Gemma3ScaledWordEmbedding(32768, 640, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm()\n",
       "          (k_norm): Gemma3RMSNorm()\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
       "          (act_fn): GELU(approximate='tanh')\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm()\n",
       "        (post_attention_layernorm): Gemma3RMSNorm()\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm()\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm()\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gemma_claude import create_gemma3_model\n",
    "\n",
    "model_claude = create_gemma3_model(vocab_size=32768)\n",
    "\n",
    "model_save_path = \"claude_model.pth\"\n",
    "# check if saved model exists\n",
    "if os.path.exists(model_save_path):\n",
    "    model_claude.load_state_dict(torch.load(model_save_path))\n",
    "    print(\"loaded model from\", model_save_path)\n",
    "else:\n",
    "    model_state = gemma_model.state_dict()\n",
    "    model_state.pop('lm_head.weight')\n",
    "    model_state.pop('model.embed_tokens.weight')\n",
    "    model_claude.load_state_dict(model_state, strict=False)\n",
    "    print(\"loaded model from gemma model\")\n",
    "    from tokenizer_matcher import TokenizerMatcher\n",
    "\n",
    "\n",
    "    tokenizer_matcher = TokenizerMatcher(\n",
    "        source_tokenizer=gemma_tokenizer,\n",
    "        target_tokenizer=tr_tokenizer,\n",
    "        source_model=gemma_model,\n",
    "        target_model=model_claude\n",
    "    )\n",
    "\n",
    "    matched_embeddings = tokenizer_matcher.match_embeddings(adding_style=\"sum\")\n",
    "    tokenizer_matcher.change_target_model_embeddings(matched_embeddings)\n",
    "    print(\"matched embeddings\")\n",
    "    # save claude model using torch \n",
    "    torch.save(model_claude.state_dict(), model_save_path)\n",
    "    print(\"saved model to\", model_save_path)\n",
    "\n",
    "model_claude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eea6750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatic device detection\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88636fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nasılsın?▁128536gi̇bi̇omurgalılarek_temp_20072kok_temp_19598kok_temp_19920kok_temp_19914ek_temp_20191553special_76kok_temp_1991619853381863kok_temp_19793kok_temp_19823kok_temp_19959kok_temp_19731295ek_temp_20108kok_temp_19706ek_temp_20229azımsabüyükanne480337ek_temp_20246kok_temp_199411872kok_temp_199781820kok_temp_195751851special_61236186128079ek_temp_20099ek_temp_20225444kok_temp_19976kok_temp_19619540kok_temp_19845kok_temp_19702'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "prompt = \"Nasılsın?\"\n",
    "\n",
    "torch.manual_seed(42)\n",
    "tokens_ids = tr_tokenizer.encode(prompt)\n",
    "outs = model_claude.generate(torch.tensor([tokens_ids]), temperature=1.0)\n",
    "# '<bos>How are you? I really enjoyed being the Muse Association Dinner Bites Venture & Innovate Guyer ceremony\\n> Got to do screens with screen mates\\n> dreamed of being pencil swift samples night\\n> like +5, picture-perfect elegance\\n'\n",
    "\n",
    "tr_tokenizer.decode(outs[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb182325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings of 157 and 30158 should be the same claude\n",
      "tensor([[ 0.4416,  0.9512,  1.0129, -0.0926, -0.5219, -1.2538, -0.1451, -1.1735,\n",
      "          1.4885, -0.5281,  1.1859, -0.0598,  0.4107,  1.3094,  0.4045]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "-------------------- gemma\n",
      "tensor([[ 0.4416,  0.9512,  1.0129, -0.0926, -0.5219, -1.2538, -0.1451, -1.1735,\n",
      "          1.4885, -0.5281,  1.1859, -0.0598,  0.4107,  1.3094,  0.4045]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "weights of any layer should be the same claude\n",
      "tensor([[-0.0170,  0.0107, -0.0175, -0.0049,  0.0083,  0.0053,  0.0072, -0.0077,\n",
      "         -0.0038,  0.0017]], grad_fn=<SliceBackward0>)\n",
      "-------------------- gemma\n",
      "tensor([[-0.0170,  0.0107, -0.0175, -0.0049,  0.0083,  0.0053,  0.0072, -0.0077,\n",
      "         -0.0038,  0.0017]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"embeddings of 157 and 30158 should be the same claude\")\n",
    "print(model_claude.model.embed_tokens(torch.tensor([157]))[:,:15])\n",
    "print(\"-\"*20, \"gemma\")\n",
    "print(gemma_model.model.embed_tokens(torch.tensor([30158]))[:,:15])\n",
    "\n",
    "print(\"weights of any layer should be the same claude\")\n",
    "print(model_claude.model.layers[0].self_attn.q_proj.weight[0:1, 0:10])\n",
    "print(\"-\"*20, \"gemma\")\n",
    "print(gemma_model.model.layers[0].self_attn.q_proj.weight[0:1, 0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61addce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target token 157 ('beniz') mapped to source tokens: [30158] (['benz'])\n",
      "Target token 30158 ('ffffff') mapped to source tokens: [62923] (['ffffff'])\n",
      "Changed 32763 embeddings out of 32763 matched tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32763"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49584def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3555]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_tokens = tokenizer_matcher.match_tokens()\n",
    "matched_tokens[2697]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac6396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<bos>Nasılsın?\\n\\n**Merhaba!**\\n\\nMerhaba, bana size nasıl yardımcı olacağını söyleyebilirim?\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_ids = gemma_tokenizer.encode(\"Nasılsın?\")\n",
    "torch.manual_seed(42)\n",
    "outs = model_claude.generate(torch.tensor([tokens_ids]), temperature=1.0)\n",
    "tr_tokenizer.decode(outs[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5697bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'raw_mediawiki', 'text'],\n",
       "    num_rows: 641443\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the English dataset from the latest dump\n",
    "ds_wiki = load_dataset(\"omarkamali/wikipedia-monthly\", \"20250703.tr\", split=\"train\")\n",
    "ds_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a8c170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1394660\n"
     ]
    }
   ],
   "source": [
    "# first 100 to text with title\n",
    "text = \"\"\n",
    "for i in range(100):\n",
    "    text += ds_wiki[i][\"title\"] + \"\\n\" + ds_wiki[i][\"text\"] + \"\\n\"\n",
    "\n",
    "while \"\\n \" in text:\n",
    "    text = text.replace(\"\\n \", \"\\n\")\n",
    "\n",
    "while \"\\n\\n\" in text:\n",
    "    text = text.replace(\"\\n\\n\", \"\\n\")\n",
    "\n",
    "\n",
    "with open(\"data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8d457a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextDataset: 2896 tokens, seq_len=512, stride=1\n",
      "Calculated sequences: 2385\n",
      "TextDataset: 577 tokens, seq_len=512, stride=1\n",
      "Calculated sequences: 66\n"
     ]
    }
   ],
   "source": [
    "from text_dataset import create_dataset, create_dataloader\n",
    "\n",
    "with open(\"data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "seq_len = 512\n",
    "stride = 1\n",
    "batch_size = 12\n",
    "shuffle = False\n",
    "\n",
    "train_dataset = create_dataset(text[:10000], tokenizer, seq_len, stride, device)\n",
    "# dataset = create_or_load_dataset(text[:50000], tr_tokenizer, seq_len, stride, device, cache_path=\"dataset/wiki1\")\n",
    "train_dataloader = create_dataloader(train_dataset, batch_size, shuffle)\n",
    "\n",
    "valid_dataset = create_dataset(text[10000:12000], tokenizer, seq_len, stride, device)\n",
    "valid_dataloader = create_dataloader(valid_dataset, batch_size, shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dbe8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Apple Silicon GPU (MPS), using MPS device\n"
     ]
    }
   ],
   "source": [
    "from gemma_trainer_claude import TrainingConfig\n",
    "\n",
    "config = TrainingConfig(\n",
    "    learning_rate=5e-5,\n",
    "    batch_size=batch_size,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_epochs=10,\n",
    "    seq_len=seq_len,\n",
    "    logging_steps=50,\n",
    "    eval_steps=200,\n",
    "    save_steps=500,\n",
    "    output_dir=\"./test_checkpoints\",\n",
    "    device=\"auto\",\n",
    "    mixed_precision=True,\n",
    "    eval_dataset_ratio=0.1,\n",
    "    do_eval=True,\n",
    "    warmup_steps=100,\n",
    "    max_grad_norm=1.0,\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825a38e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 21:34:01,048 - gemma_trainer_claude - INFO - Using Apple Silicon GPU (MPS)\n",
      "2025-08-17 21:34:01,049 - gemma_trainer_claude - INFO - Mixed precision training enabled (MPS)\n",
      "2025-08-17 21:34:01,050 - gemma_trainer_claude - INFO - Model moved to mps\n",
      "2025-08-17 21:34:01,051 - gemma_trainer_claude - INFO - Total parameters: 435,870,336\n",
      "2025-08-17 21:34:01,051 - gemma_trainer_claude - INFO - Trainable parameters: 435,870,336\n",
      "2025-08-17 21:34:01,053 - gemma_trainer_claude - INFO - Optimizer: adamw\n",
      "2025-08-17 21:34:01,053 - gemma_trainer_claude - INFO - Scheduler: cosine_with_warmup\n",
      "2025-08-17 21:34:01,053 - gemma_trainer_claude - INFO - Total training steps: 990\n"
     ]
    }
   ],
   "source": [
    "from gemma_trainer_claude import create_trainer\n",
    "\n",
    "\n",
    "trainer = create_trainer(\n",
    "        model=model_claude,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        config=config,\n",
    "        tokenizer=tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce2259f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 21:35:17,329 - gemma_trainer_claude - INFO - Starting training...\n",
      "2025-08-17 21:35:17,333 - gemma_trainer_claude - INFO - Training config: {'model_name': 'gemma3', 'learning_rate': 5e-05, 'min_learning_rate': 1e-06, 'weight_decay': 0.01, 'max_epochs': 10, 'max_steps': None, 'warmup_steps': 100, 'warmup_ratio': 0.1, 'seq_len': 512, 'batch_size': 12, 'gradient_accumulation_steps': 2, 'max_grad_norm': 1.0, 'logging_steps': 50, 'eval_steps': 200, 'save_steps': 500, 'save_total_limit': 3, 'output_dir': './test_checkpoints', 'logging_dir': './test_checkpoints/logs', 'device': 'mps', 'mixed_precision': True, 'compile_model': False, 'eval_dataset_ratio': 0.1, 'do_eval': True, 'optimizer_type': 'adamw', 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-08, 'scheduler_type': 'cosine_with_warmup'}\n",
      "2025-08-17 21:35:17,339 - gemma_trainer_claude - INFO - Epoch 1/10\n",
      "/Users/alibayram/.pyenv/versions/3.13.3/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Interrupted by User ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 21:44:15,901 - gemma_trainer_claude - INFO - Checkpoint saved to ./test_checkpoints/interrupted_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to: ./test_checkpoints/interrupted_model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"\\n=== Training Completed Successfully! ===\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n=== Training Interrupted by User ===\")\n",
    "    # Save final checkpoint\n",
    "    final_checkpoint_dir = os.path.join(\"./test_checkpoints\", \"interrupted_model\")\n",
    "    trainer.save_checkpoint(final_checkpoint_dir)\n",
    "    print(f\"Checkpoint saved to: {final_checkpoint_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n=== Training Failed: {e} ===\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0eff77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: turkish-tokenizer in /Users/alibayram/.pyenv/versions/3.13.3/lib/python3.13/site-packages (0.2.13)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install turkish-tokenizer -U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0182b203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google/gemma-3-270m-it'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2ea392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e096f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cdb55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF model vocab size: 262144, Current model vocab size: 262144\n",
      "Missing keys: ['local_freqs_cis', 'global_freqs_cis']\n",
      "Successfully loaded 236 weights\n"
     ]
    }
   ],
   "source": [
    "from gemma_model import GemmaForCausalLM, get_config_for_270m\n",
    "\n",
    "config_270m = get_config_for_270m(\"float32\")\n",
    "\n",
    "gemma_model = GemmaForCausalLM(config_270m, tokenizer)\n",
    "gemma_model.load_weights_from_hf(model.model.state_dict())\n",
    "# gemma_model.from_pretrained(\"gemma-3-270m-hf-it\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eed2250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4189, -0.0205, -1.4623,  0.2724,  0.2938, -0.0337, -1.2584,  2.4883]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2938])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "min_prompt_len = 5\n",
    "output_positions_tensor = torch.tensor([min_prompt_len - 1], dtype=torch.long)\n",
    "\n",
    "hidden_states = torch.randn(1, 8)\n",
    "print(hidden_states)\n",
    "hidden_states = hidden_states.index_select(1, output_positions_tensor).squeeze(dim=1)\n",
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee80eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (embedder): Embedding(262144, 640)\n",
       "  (model): GemmaModel(\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
       "          (query_norm): RMSNorm()\n",
       "          (key_norm): RMSNorm()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (pre_feedforward_layernorm): RMSNorm()\n",
       "        (post_feedforward_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8223ba95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['local_freqs_cis', 'global_freqs_cis', 'model.embedder.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.query_norm.weight', 'model.layers.0.self_attn.key_norm.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.0.pre_feedforward_layernorm.weight', 'model.layers.0.post_feedforward_layernorm.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.query_norm.weight', 'model.layers.1.self_attn.key_norm.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.1.pre_feedforward_layernorm.weight', 'model.layers.1.post_feedforward_layernorm.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.query_norm.weight', 'model.layers.2.self_attn.key_norm.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.2.pre_feedforward_layernorm.weight', 'model.layers.2.post_feedforward_layernorm.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.query_norm.weight', 'model.layers.3.self_attn.key_norm.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.3.pre_feedforward_layernorm.weight', 'model.layers.3.post_feedforward_layernorm.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.query_norm.weight', 'model.layers.4.self_attn.key_norm.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.4.pre_feedforward_layernorm.weight', 'model.layers.4.post_feedforward_layernorm.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.query_norm.weight', 'model.layers.5.self_attn.key_norm.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.5.pre_feedforward_layernorm.weight', 'model.layers.5.post_feedforward_layernorm.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.query_norm.weight', 'model.layers.6.self_attn.key_norm.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.6.pre_feedforward_layernorm.weight', 'model.layers.6.post_feedforward_layernorm.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.query_norm.weight', 'model.layers.7.self_attn.key_norm.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.7.pre_feedforward_layernorm.weight', 'model.layers.7.post_feedforward_layernorm.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.query_norm.weight', 'model.layers.8.self_attn.key_norm.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.8.pre_feedforward_layernorm.weight', 'model.layers.8.post_feedforward_layernorm.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.query_norm.weight', 'model.layers.9.self_attn.key_norm.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.9.pre_feedforward_layernorm.weight', 'model.layers.9.post_feedforward_layernorm.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.query_norm.weight', 'model.layers.10.self_attn.key_norm.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.10.pre_feedforward_layernorm.weight', 'model.layers.10.post_feedforward_layernorm.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.query_norm.weight', 'model.layers.11.self_attn.key_norm.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.11.pre_feedforward_layernorm.weight', 'model.layers.11.post_feedforward_layernorm.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.query_norm.weight', 'model.layers.12.self_attn.key_norm.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.12.pre_feedforward_layernorm.weight', 'model.layers.12.post_feedforward_layernorm.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.query_norm.weight', 'model.layers.13.self_attn.key_norm.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.13.pre_feedforward_layernorm.weight', 'model.layers.13.post_feedforward_layernorm.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.query_norm.weight', 'model.layers.14.self_attn.key_norm.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.14.pre_feedforward_layernorm.weight', 'model.layers.14.post_feedforward_layernorm.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.query_norm.weight', 'model.layers.15.self_attn.key_norm.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.15.pre_feedforward_layernorm.weight', 'model.layers.15.post_feedforward_layernorm.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.query_norm.weight', 'model.layers.16.self_attn.key_norm.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.16.pre_feedforward_layernorm.weight', 'model.layers.16.post_feedforward_layernorm.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.query_norm.weight', 'model.layers.17.self_attn.key_norm.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.17.pre_feedforward_layernorm.weight', 'model.layers.17.post_feedforward_layernorm.weight', 'model.norm.weight'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bdfcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Merhaba, nasılsın?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b6b6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af31d387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nBen, \"Öyle bir şey bulmak istiyorsan, bir arkadaşım veya bir ofis çalışanının yanında bulunabilirsin. Konuşmak için çok mu hızlı bir şekilde ve kolay bir şekilde, kısa süzen bir zaman ardında, olumlu bir ortamda çalışabilirsin?\"\\n\\nBu metni kendi özür ifadeleriyle sunun.\\nBu komutu farklı bir konuma veya ihtiyaçlara göre uyarlayabilirsiniz.\\nÖ puissiezize veya ihtiyacınızın ne'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_model.generate(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e590b486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to gemma-3-270m-hf-it\n"
     ]
    }
   ],
   "source": [
    "gemma_model.save_pretrained(\"gemma-3-270m-hf-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b213325c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [3, 0, 2697, 2, 0, 2212, 2, 0, 2794, 1, 2, 18194, 20043, 2, 6766, 20000, 2, 17321, 20000, 2, 20024, 2, 2595, 20024, 2, 2627, 2, 20024, 2, 3045, 20024, 2, 227, 20024, 2, 227, 15247, 2, 2656, 10572, 2, 2503, 2, 2599, 20038, 3, 3, 2, 165, 20021, 20035, 2, 20064, 4373, 20002, 3, 0, 165, 2, 165, 20037, 2, 2501, 2, 3303, 4, 2502, 20026, 3, 2]\n",
      "Decoded: \n",
      "Ali Ata Bak▁u▁ aliler ahmetler selmanlar da bizde onlar da testte kitapta kitabını okudum bu işe\n",
      "\n",
      " burnunu sokma\n",
      "Burun burna bir kaza\toldu\n",
      " \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Metin tokenizasyonu\n",
    "text = \"\"\"\n",
    "Ali Ata Bak▁ aliler ahmetler selmanlar da bizde onlar da testte kitapta kitabını okudum bu işe\n",
    " \n",
    " burnunu sokma\n",
    "Burun buruna bir kaza\\toldu\n",
    " \"\"\"\n",
    "tokens = tr_tokenizer.encode(text)\n",
    "print(\"Token IDs:\", tokens)\n",
    "\n",
    "# Token'ları metne geri çevir\n",
    "decoded_text = tr_tokenizer.decode(tokens)\n",
    "print(\"Decoded:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b706db8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_dataset import create_dataset, create_dataloader\n",
    "\n",
    "with open(\"data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "seq_len = 512\n",
    "stride = 1\n",
    "batch_size = 12\n",
    "shuffle = False\n",
    "\n",
    "train_dataset = create_dataset(text[:10000], tr_tokenizer, seq_len, stride, device)\n",
    "# dataset = create_or_load_dataset(text[:50000], tr_tokenizer, seq_len, stride, device, cache_path=\"dataset/wiki1\")\n",
    "train_dataloader = create_dataloader(train_dataset, batch_size, shuffle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb5c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = create_dataset(text[10000:12000], tr_tokenizer, seq_len, stride, device)\n",
    "valid_dataloader = create_dataloader(valid_dataset, batch_size, shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decfd09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'seq_len': 512,\n",
       "  'stride': 1,\n",
       "  'num_sequences': 3875,\n",
       "  'tokenizer_name': 'unknown'},\n",
       " {'seq_len': 512,\n",
       "  'stride': 1,\n",
       "  'num_sequences': 503,\n",
       "  'tokenizer_name': 'unknown'})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.get_sequence_info(), valid_dataset.get_sequence_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e29352c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Missing keys when loading model: ['model.embedder.weight']\n",
      "Warning: Unexpected keys when loading model: ['embedder.weight']\n",
      "Model loaded from gemma-3-270m-tr-tokenizer-it\n"
     ]
    }
   ],
   "source": [
    "from gemma_model import GemmaForCausalLM, get_config_for_270m_tr_tokenizer\n",
    "\n",
    "\n",
    "config_270m = get_config_for_270m_tr_tokenizer(\"float32\")\n",
    "\n",
    "gemma_model = GemmaForCausalLM(config_270m, tr_tokenizer, device)\n",
    "# gemma_model.load_weights_from_hf(model.model.state_dict())\n",
    "gemma_model.from_pretrained(\"gemma-3-270m-tr-tokenizer-it\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f678e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embedder): Embedding(32768, 640)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
       "          (query_norm): RMSNorm()\n",
       "          (key_norm): RMSNorm()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (pre_feedforward_layernorm): RMSNorm()\n",
       "        (post_feedforward_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793f226c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/10 | Train Loss: 0.5769 | Val Loss: 13.9190 | LR: 0.001000\n",
      "Validation loss decreased (inf --> 13.9190). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 3. Instantiate and Run the Trainer\u001b[39;00m\n\u001b[32m     11\u001b[39m trainer = ModelTrainer(\n\u001b[32m     12\u001b[39m     model=gemma_model,\n\u001b[32m     13\u001b[39m     train_loader=train_dataloader,\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     patience=\u001b[32m3\u001b[39m\n\u001b[32m     21\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTrainer has finished. The `trainer.model` now holds the best weights.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llms/embedding/model_trainer.py:108\u001b[39m, in \u001b[36mModelTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting training on device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.epochs):\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     train_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m     val_loss = \u001b[38;5;28mself\u001b[39m._validate_epoch()\n\u001b[32m    111\u001b[39m     current_lr = \u001b[38;5;28mself\u001b[39m.optimizer.param_groups[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llms/embedding/model_trainer.py:74\u001b[39m, in \u001b[36mModelTrainer._train_epoch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     71\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.criterion(outputs.view(-\u001b[32m1\u001b[39m, outputs.size(-\u001b[32m1\u001b[39m)), targets.view(-\u001b[32m1\u001b[39m))\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Gradient Clipping to prevent exploding gradients\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.grad_clip_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from model_trainer import ModelTrainer\n",
    "\n",
    "optimizer = optim.AdamW(gemma_model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# 3. Instantiate and Run the Trainer\n",
    "trainer = ModelTrainer(\n",
    "    model=gemma_model,\n",
    "    train_loader=train_dataloader,\n",
    "    val_loader=valid_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    epochs=10,\n",
    "    patience=3\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nTrainer has finished. The `trainer.model` now holds the best weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6d3310",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "index_copy_() received an invalid combination of arguments - got (int, NoneType, Tensor), but expected one of:\n * (int dim, Tensor index, Tensor source)\n      didn't match because some of the arguments have invalid types: (int, !NoneType!, Tensor)\n * (name dim, Tensor index, Tensor source)\n      didn't match because some of the arguments have invalid types: (!int!, !NoneType!, Tensor)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgemma_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llms/embedding/gemma_model.py:620\u001b[39m, in \u001b[36mGemmaForCausalLM.generate\u001b[39m\u001b[34m(self, prompts, output_len, temperature, top_p, top_k)\u001b[39m\n\u001b[32m    617\u001b[39m \u001b[38;5;66;03m# Prefill up to min_prompt_len tokens, then treat other prefill as\u001b[39;00m\n\u001b[32m    618\u001b[39m \u001b[38;5;66;03m# decode and ignore output.\u001b[39;00m\n\u001b[32m    619\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_seq_len - min_prompt_len):\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m   next_token_ids, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_token_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_token_ids_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_positions\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_positions_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkv_write_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurr_mask_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_positions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_positions_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperatures_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtop_ps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_ps_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtop_ks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_ks_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurr_local_mask_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    633\u001b[39m   curr_prompt_mask = prompt_mask_tensor.index_select(\n\u001b[32m    634\u001b[39m             \u001b[32m1\u001b[39m, output_index).squeeze(dim=\u001b[32m1\u001b[39m)\n\u001b[32m    635\u001b[39m   curr_token_ids = token_ids_tensor.index_select(\n\u001b[32m    636\u001b[39m             \u001b[32m1\u001b[39m, output_index).squeeze(dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llms/embedding/gemma_model.py:508\u001b[39m, in \u001b[36mGemmaForCausalLM.forward\u001b[39m\u001b[34m(self, input_token_ids, input_positions, kv_write_indices, kv_caches, mask, output_positions, temperatures, top_ps, top_ks, local_mask, **kwargs)\u001b[39m\n\u001b[32m    501\u001b[39m freqs_cis[AttentionType.LOCAL_SLIDING] = (\n\u001b[32m    502\u001b[39m     \u001b[38;5;28mself\u001b[39m.local_freqs_cis.index_select(\u001b[32m0\u001b[39m, positions)\n\u001b[32m    503\u001b[39m )\n\u001b[32m    504\u001b[39m freqs_cis[AttentionType.GLOBAL] = (\n\u001b[32m    505\u001b[39m     \u001b[38;5;28mself\u001b[39m.global_freqs_cis.index_select(\u001b[32m0\u001b[39m, positions)\n\u001b[32m    506\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_token_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_token_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkv_write_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_write_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;66;03m# For training, return logits for the entire sequence\u001b[39;00m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_positions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llms/embedding/gemma_model.py:395\u001b[39m, in \u001b[36mGemmaModel.forward\u001b[39m\u001b[34m(self, input_token_ids, freqs_cis, kv_write_indices, kv_caches, mask, local_mask)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.layers)):\n\u001b[32m    394\u001b[39m     layer = \u001b[38;5;28mself\u001b[39m.layers[i]\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     hidden_states = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattn_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkv_write_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_write_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llms/embedding/gemma_model.py:338\u001b[39m, in \u001b[36mGemmaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, freqs_cis, kv_write_indices, kv_cache, mask, local_mask)\u001b[39m\n\u001b[32m    336\u001b[39m residual = hidden_states\n\u001b[32m    337\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkv_write_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_write_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m    347\u001b[39m hidden_states = residual + hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llms/embedding/gemma_model.py:249\u001b[39m, in \u001b[36mGemmaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, freqs_cis, kv_write_indices, kv_cache, mask, local_mask)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# Write new kv cache.\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[38;5;66;03m# [batch_size, input_len, n_local_kv_heads, head_dim]\u001b[39;00m\n\u001b[32m    248\u001b[39m k_cache, v_cache = kv_cache\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[43mk_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex_copy_\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_write_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m v_cache.index_copy_(\u001b[32m1\u001b[39m, kv_write_indices, xv)\n\u001b[32m    252\u001b[39m key = k_cache\n",
      "\u001b[31mTypeError\u001b[39m: index_copy_() received an invalid combination of arguments - got (int, NoneType, Tensor), but expected one of:\n * (int dim, Tensor index, Tensor source)\n      didn't match because some of the arguments have invalid types: (int, !NoneType!, Tensor)\n * (name dim, Tensor index, Tensor source)\n      didn't match because some of the arguments have invalid types: (!int!, !NoneType!, Tensor)\n"
     ]
    }
   ],
   "source": [
    "gemma_model.generate(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9201cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate text (no device parameter needed)\n",
    "response = gemma_model.generate(\n",
    "    prompts=\"Hello, how are you?\",\n",
    "    output_len=50,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d48c4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ek_temp_20212ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff85a5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ek_temp_20200ek_temp_20084ek_temp_20074ek_temp_20074ek_temp_20074ek_temp_20074ek_temp_20074ek_temp_20074ek_temp_20074ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_model.generate(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6288685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7912,  0.1602,  0.1088, -2.0236,  1.2162, -0.8047, -0.3950,  0.3219,\n",
       "         -0.0877,  0.2307,  0.3475,  0.3288,  0.6641,  1.6956, -0.2081]],\n",
       "       device='mps:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "gemma_model.model.embedder(torch.tensor([157]).to(gemma_model.device))[:,:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f011a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32768, 640])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_model.embedder.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdec085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4416,  0.9512,  1.0129, -0.0926, -0.5219, -1.2538, -0.1451, -1.1735,\n",
       "          1.4885, -0.5281,  1.1859, -0.0598,  0.4107,  1.3094,  0.4045]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.embed_tokens(torch.tensor([30158]))[:,:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7c02ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0264, -0.0087,  0.0054,  0.0137, -0.0089,  0.0084,  0.0043, -0.0322,\n",
       "          0.0408,  0.0034, -0.0100,  0.0056, -0.0010,  0.0203, -0.0325],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " tensor([ 0.0264, -0.0087,  0.0054,  0.0137, -0.0089,  0.0084,  0.0043, -0.0322,\n",
       "          0.0408,  0.0034, -0.0100,  0.0056, -0.0010,  0.0203, -0.0325],\n",
       "        device='mps:0', grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[4].mlp.gate_proj.weight[234][:15], gemma_model.model.layers[4].mlp.gate_proj.weight[234][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60b9234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to gemma-3-270m-tr-tokenizer-it\n"
     ]
    }
   ],
   "source": [
    "gemma_model.save_pretrained(\"gemma-3-270m-tr-tokenizer-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7490419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tensors = tokenizer.encode(prompt)\n",
    "tensors = torch.tensor(tensors)\n",
    "tensors = tensors.unsqueeze(0)\n",
    "tensors = tensors.to(\"cpu\")\n",
    "\n",
    "ids = model.generate(tensors)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54019b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509528d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1b = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-pt\")\n",
    "model_1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d46f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tr_tokens = tr_tokenizer.get_vocab()\n",
    "\n",
    "print(len(all_tr_tokens))\n",
    "print(len(tr_tokenizer.reverse_dict))\n",
    "print(tr_tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4235ab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tr_tokens[\"ali\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f9c10b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e0b5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed 32763 embeddings out of 32763 matched tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ek_temp_20200ek_temp_20084ek_temp_20074ek_temp_20074ek_temp_20074ek_temp_20074ek_temp_20074ek_temp_20074ek_temp_20074ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizer_matcher import TokenizerMatcher\n",
    "\n",
    "\n",
    "tokenizer_matcher = TokenizerMatcher(tokenizer, tr_tokenizer, model, gemma_model)\n",
    "matched_embeddings = tokenizer_matcher.match_embeddings()\n",
    "\n",
    "tokenizer_matcher.change_target_model_embeddings(matched_embeddings)\n",
    "\n",
    "gemma_model.generate(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff9ac5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 30158]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_tokens = tokenizer_matcher.match_tokens()\n",
    "matched_tokens[157]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e99dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262144"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd445ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"soğucuk\"\n",
    "ids = tokenizer.encode(token)\n",
    "tr_ids = tr_tokenizer.encode(token)\n",
    "\n",
    "print(ids)\n",
    "print(tr_ids)\n",
    "\n",
    "print(tokenizer.tokenize(token))\n",
    "print(tr_tokenizer.tokenize(token))\n",
    "print(tr_tokenizer.decode(tr_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d4214",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_ids = [157, 165, 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac7a399",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958463f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.state_dict().keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645831b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0175,  0.0376,  0.0400, -0.0037, -0.0206, -0.0496, -0.0057, -0.0464,\n",
       "         0.0588, -0.0209,  0.0469, -0.0024,  0.0162,  0.0518,  0.0160],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.embed_tokens.weight[30158][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833c65a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0175,  0.0376,  0.0400, -0.0037, -0.0206, -0.0496, -0.0057, -0.0464,\n",
       "         0.0588, -0.0209,  0.0469, -0.0024,  0.0162,  0.0518,  0.0160],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.weight[30158][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512556d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
