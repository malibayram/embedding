{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb5697bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'raw_mediawiki', 'text'],\n",
       "    num_rows: 641443\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the English dataset from the latest dump\n",
    "ds_wiki = load_dataset(\"omarkamali/wikipedia-monthly\", \"20250703.tr\", split=\"train\")\n",
    "ds_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23a8c170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1394660\n"
     ]
    }
   ],
   "source": [
    "# first 100 to text with title\n",
    "text = \"\"\n",
    "for i in range(100):\n",
    "    text += ds_wiki[i][\"title\"] + \"\\n\" + ds_wiki[i][\"text\"] + \"\\n\"\n",
    "\n",
    "while \"\\n \" in text:\n",
    "    text = text.replace(\"\\n \", \"\\n\")\n",
    "\n",
    "while \"\\n\\n\" in text:\n",
    "    text = text.replace(\"\\n\\n\", \"\\n\")\n",
    "\n",
    "\n",
    "with open(\"data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d0eff77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: turkish-tokenizer in /Users/alibayram/.pyenv/versions/3.13.3/lib/python3.13/site-packages (0.2.13)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install turkish-tokenizer -U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0182b203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-270m-it\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea2ea392",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-270m-it\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54cdb55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from gemma-3-270m-hf-it\n"
     ]
    }
   ],
   "source": [
    "from gemma_model import GemmaForCausalLM, get_config_for_270m\n",
    "\n",
    "config_270m = get_config_for_270m(\"float32\")\n",
    "\n",
    "gemma_model = GemmaForCausalLM(config_270m, tokenizer)\n",
    "# gemma_model.load_weights_from_hf(model.model.state_dict())\n",
    "gemma_model.from_pretrained(\"gemma-3-270m-hf-it\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19bdfcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Merhaba, nasılsın?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af31d387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Bugün ne yaptın?\\n\\n**Merhaba!**\\n\\n**Neden?**\\n\\n\"Merhaba, nasılsın?\"\\n\\nBu, birçok farklı alanda (aile, arkadaşlıklar, iş, vb.) kullanımıyla ilişkilendirilebilir. Bu ifade, \"Merhaba\" kelimesiyle ilgili bir farklılık veya kelime farkındalığı duygusunu ifade eder.\\n\\n**Ne yazılabilir?**\\n\\nBu ifade, \"Merhaba\" kelimesiyle ilgili bir farklılık veya'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_model.generate(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e590b486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to gemma-3-270m-hf-it\n"
     ]
    }
   ],
   "source": [
    "gemma_model.save_pretrained(\"gemma-3-270m-hf-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b213325c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [3, 0, 2697, 2, 0, 2212, 2, 0, 2794, 1, 2, 18194, 20043, 2, 6766, 20000, 2, 17321, 20000, 2, 20024, 2, 2595, 20024, 2, 2627, 2, 20024, 2, 3045, 20024, 2, 227, 20024, 2, 227, 15247, 2, 2656, 10572, 2, 2503, 2, 2599, 20038, 3, 3, 2, 165, 20021, 20035, 2, 20064, 4373, 20002, 3, 0, 165, 2, 165, 20037, 2, 2501, 2, 3303, 4, 2502, 20026, 3, 2]\n",
      "Decoded: \n",
      "Ali Ata Bak▁u▁ aliler ahmetler selmanlar da bizde onlar da testte kitapta kitabını okudum bu işe\n",
      "\n",
      " burnunu sokma\n",
      "Burun burna bir kaza\toldu\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from turkish_tokenizer import TurkishTokenizer\n",
    "\n",
    "# Tokenizer'ı başlat\n",
    "tr_tokenizer = TurkishTokenizer()\n",
    "\n",
    "# Metin tokenizasyonu\n",
    "text = \"\"\"\n",
    "Ali Ata Bak▁ aliler ahmetler selmanlar da bizde onlar da testte kitapta kitabını okudum bu işe\n",
    " \n",
    " burnunu sokma\n",
    "Burun buruna bir kaza\\toldu\n",
    " \"\"\"\n",
    "tokens = tr_tokenizer.encode(text)\n",
    "print(\"Token IDs:\", tokens)\n",
    "\n",
    "# Token'ları metne geri çevir\n",
    "decoded_text = tr_tokenizer.decode(tokens)\n",
    "print(\"Decoded:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e29352c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from gemma-3-270m-tr-tokenizer-it\n"
     ]
    }
   ],
   "source": [
    "from gemma_model import GemmaForCausalLM, get_config_for_270m_tr_tokenizer\n",
    "import torch\n",
    "# Automatic device detection\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "config_270m = get_config_for_270m_tr_tokenizer(\"float32\")\n",
    "\n",
    "gemma_model = GemmaForCausalLM(config_270m, tr_tokenizer, device)\n",
    "# gemma_model.load_weights_from_hf(model.model.state_dict())\n",
    "gemma_model.from_pretrained(\"gemma-3-270m-tr-tokenizer-it\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5e7010",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed9201cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate text (no device parameter needed)\n",
    "response = gemma_model.generate(\n",
    "    prompts=\"Hello, how are you?\",\n",
    "    output_len=50,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d48c4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ek_temp_20212ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244ek_temp_20244'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff85a5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ek_temp_20200ek_temp_20084ek_temp_20074ek_temp_20074ek_temp_20074ek_temp_20074ek_temp_20074ek_temp_20074ek_temp_20074ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_model.generate(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6288685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4416,  0.9512,  1.0129, -0.0926, -0.5219, -1.2538, -0.1451, -1.1735,\n",
       "          1.4885, -0.5281,  1.1859, -0.0598,  0.4107,  1.3094,  0.4045]],\n",
       "       device='mps:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "gemma_model.embedder(torch.tensor([157]).to(gemma_model.device))[:,:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "72f011a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32768, 640])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_model.embedder.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfdec085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4416,  0.9512,  1.0129, -0.0926, -0.5219, -1.2538, -0.1451, -1.1735,\n",
       "          1.4885, -0.5281,  1.1859, -0.0598,  0.4107,  1.3094,  0.4045]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.embed_tokens(torch.tensor([30158]))[:,:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c7c02ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0264, -0.0087,  0.0054,  0.0137, -0.0089,  0.0084,  0.0043, -0.0322,\n",
       "          0.0408,  0.0034, -0.0100,  0.0056, -0.0010,  0.0203, -0.0325],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " tensor([ 0.0264, -0.0087,  0.0054,  0.0137, -0.0089,  0.0084,  0.0043, -0.0322,\n",
       "          0.0408,  0.0034, -0.0100,  0.0056, -0.0010,  0.0203, -0.0325],\n",
       "        grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[4].mlp.gate_proj.weight[234][:15], gemma_model.model.layers[4].mlp.gate_proj.weight[234][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e60b9234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to gemma-3-270m-tr-tokenizer-it\n"
     ]
    }
   ],
   "source": [
    "gemma_model.save_pretrained(\"gemma-3-270m-tr-tokenizer-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7490419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tensors = tokenizer.encode(prompt)\n",
    "tensors = torch.tensor(tensors)\n",
    "tensors = tensors.unsqueeze(0)\n",
    "tensors = tensors.to(\"cpu\")\n",
    "\n",
    "ids = model.generate(tensors)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54019b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509528d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1b = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-pt\")\n",
    "model_1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d46f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tr_tokens = tr_tokenizer.get_vocab()\n",
    "\n",
    "print(len(all_tr_tokens))\n",
    "print(len(tr_tokenizer.reverse_dict))\n",
    "print(tr_tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4235ab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tr_tokens[\"ali\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f9c10b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8e0b5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed 32763 embeddings out of 32763 matched tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ek_temp_20200ek_temp_20084ek_temp_20074ek_temp_20074ek_temp_20074ek_temp_20074ek_temp_20074ek_temp_20074ek_temp_20074ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077ek_temp_20077'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizer_matcher import TokenizerMatcher\n",
    "\n",
    "\n",
    "tokenizer_matcher = TokenizerMatcher(tokenizer, tr_tokenizer, model, gemma_model)\n",
    "matched_embeddings = tokenizer_matcher.match_embeddings()\n",
    "\n",
    "tokenizer_matcher.change_target_model_embeddings(matched_embeddings)\n",
    "\n",
    "gemma_model.generate(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ff9ac5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 30158]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_tokens = tokenizer_matcher.match_tokens()\n",
    "matched_tokens[157]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23e99dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262144"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd445ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"soğucuk\"\n",
    "ids = tokenizer.encode(token)\n",
    "tr_ids = tr_tokenizer.encode(token)\n",
    "\n",
    "print(ids)\n",
    "print(tr_ids)\n",
    "\n",
    "print(tokenizer.tokenize(token))\n",
    "print(tr_tokenizer.tokenize(token))\n",
    "print(tr_tokenizer.decode(tr_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d4214",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_ids = [157, 165, 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac7a399",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958463f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.state_dict().keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "645831b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0175,  0.0376,  0.0400, -0.0037, -0.0206, -0.0496, -0.0057, -0.0464,\n",
       "         0.0588, -0.0209,  0.0469, -0.0024,  0.0162,  0.0518,  0.0160],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.embed_tokens.weight[30158][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "833c65a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0175,  0.0376,  0.0400, -0.0037, -0.0206, -0.0496, -0.0057, -0.0464,\n",
       "         0.0588, -0.0209,  0.0469, -0.0024,  0.0162,  0.0518,  0.0160],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.weight[30158][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512556d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
